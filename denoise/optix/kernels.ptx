//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-34714021
// Cuda compilation tools, release 12.6, V12.6.68
// Based on NVVM 7.0.1
//

.version 8.5
.target sm_50
.address_size 64

	// .globl	kernelLoad

.visible .entry kernelLoad(
	.param .u32 kernelLoad_param_0,
	.param .u32 kernelLoad_param_1,
	.param .u64 kernelLoad_param_2,
	.param .u64 kernelLoad_param_3,
	.param .u64 kernelLoad_param_4,
	.param .u64 kernelLoad_param_5,
	.param .u64 kernelLoad_param_6,
	.param .u32 kernelLoad_param_7,
	.param .u64 kernelLoad_param_8,
	.param .u64 kernelLoad_param_9,
	.param .u64 kernelLoad_param_10,
	.param .u64 kernelLoad_param_11
)
{
	.reg .pred 	%p<8>;
	.reg .b16 	%rs<23>;
	.reg .f32 	%f<39>;
	.reg .b32 	%r<20>;
	.reg .b64 	%rd<25>;


	ld.param.u32 	%r3, [kernelLoad_param_0];
	ld.param.u32 	%r4, [kernelLoad_param_1];
	ld.param.u64 	%rd2, [kernelLoad_param_2];
	ld.param.u64 	%rd3, [kernelLoad_param_3];
	ld.param.u64 	%rd4, [kernelLoad_param_4];
	ld.param.u64 	%rd5, [kernelLoad_param_5];
	ld.param.u64 	%rd6, [kernelLoad_param_6];
	ld.param.u32 	%r5, [kernelLoad_param_7];
	ld.param.u64 	%rd7, [kernelLoad_param_8];
	ld.param.u64 	%rd8, [kernelLoad_param_9];
	ld.param.u64 	%rd9, [kernelLoad_param_10];
	ld.param.u64 	%rd10, [kernelLoad_param_11];
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r8, %tid.x;
	mad.lo.s32 	%r1, %r7, %r6, %r8;
	mov.u32 	%r9, %ntid.y;
	mov.u32 	%r10, %ctaid.y;
	mov.u32 	%r11, %tid.y;
	mad.lo.s32 	%r2, %r10, %r9, %r11;
	setp.ge.u32 	%p1, %r1, %r3;
	setp.ge.u32 	%p2, %r2, %r4;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB0_9;

	shl.b32 	%r12, %r1, 3;
	suld.b.2d.v4.b16.trap {%rs5, %rs6, %rs7, %rs8}, [%rd2, {%r12, %r2}];
	// begin inline asm
	{  cvt.f32.f16 %f10, %rs5;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f11, %rs6;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f12, %rs7;}

	// end inline asm
	mov.f32 	%f13, 0f461C4000;
	min.f32 	%f14, %f10, %f13;
	mov.f32 	%f15, 0f00000000;
	max.f32 	%f16, %f15, %f14;
	min.f32 	%f17, %f11, %f13;
	max.f32 	%f18, %f15, %f17;
	min.f32 	%f19, %f12, %f13;
	max.f32 	%f20, %f15, %f19;
	mad.lo.s32 	%r13, %r2, %r3, %r1;
	cvt.u64.u32 	%rd1, %r13;
	cvta.to.global.u64 	%rd11, %rd3;
	mul.wide.u32 	%rd12, %r13, 12;
	add.s64 	%rd13, %rd11, %rd12;
	st.global.f32 	[%rd13], %f16;
	st.global.f32 	[%rd13+4], %f18;
	st.global.f32 	[%rd13+8], %f20;
	setp.eq.s64 	%p4, %rd4, 0;
	@%p4 bra 	$L__BB0_5;

	shl.b32 	%r14, %r1, 2;
	mov.u32 	%r15, 0;
	suld.b.a2d.v4.b8.trap {%rs1, %rs2, %rs3, %rs4}, [%rd4, {%r15, %r14, %r2, %r2}];
	mov.u32 	%r16, 1;
	suld.b.a2d.v4.b8.trap {%rs9, %rs10, %rs11, %rs12}, [%rd4, {%r16, %r14, %r2, %r2}];
	and.b16  	%rs13, %rs9, 255;
	and.b16  	%rs14, %rs10, 255;
	and.b16  	%rs15, %rs11, 255;
	cvt.rn.f32.u16 	%f21, %rs15;
	mul.f32 	%f36, %f21, 0f3B808081;
	cvt.rn.f32.u16 	%f22, %rs14;
	mul.f32 	%f37, %f22, 0f3B808081;
	cvt.rn.f32.u16 	%f23, %rs13;
	mul.f32 	%f38, %f23, 0f3B808081;
	setp.eq.s32 	%p5, %r5, 0;
	@%p5 bra 	$L__BB0_4;

	fma.rn.f32 	%f36, %f36, 0f40000000, 0fBF800000;
	fma.rn.f32 	%f37, %f37, 0f40000000, 0fBF800000;
	fma.rn.f32 	%f38, %f38, 0f40000000, 0fBF800000;

$L__BB0_4:
	cvta.to.global.u64 	%rd14, %rd5;
	mul.lo.s64 	%rd15, %rd1, 12;
	add.s64 	%rd16, %rd14, %rd15;
	and.b16  	%rs16, %rs3, 255;
	cvt.rn.f32.u16 	%f24, %rs16;
	mul.f32 	%f25, %f24, 0f3B808081;
	st.global.f32 	[%rd16], %f25;
	and.b16  	%rs17, %rs2, 255;
	cvt.rn.f32.u16 	%f26, %rs17;
	mul.f32 	%f27, %f26, 0f3B808081;
	st.global.f32 	[%rd16+4], %f27;
	and.b16  	%rs18, %rs1, 255;
	cvt.rn.f32.u16 	%f28, %rs18;
	mul.f32 	%f29, %f28, 0f3B808081;
	st.global.f32 	[%rd16+8], %f29;
	cvta.to.global.u64 	%rd17, %rd6;
	add.s64 	%rd18, %rd17, %rd15;
	st.global.f32 	[%rd18], %f36;
	st.global.f32 	[%rd18+4], %f37;
	st.global.f32 	[%rd18+8], %f38;

$L__BB0_5:
	setp.eq.s64 	%p6, %rd7, 0;
	@%p6 bra 	$L__BB0_7;

	shl.b32 	%r17, %r1, 2;
	suld.b.2d.b32.trap {%r18}, [%rd7, {%r17, %r2}];
	cvta.to.global.u64 	%rd19, %rd8;
	shl.b64 	%rd20, %rd1, 2;
	add.s64 	%rd21, %rd19, %rd20;
	st.global.u32 	[%rd21], %r18;

$L__BB0_7:
	setp.eq.s64 	%p7, %rd9, 0;
	@%p7 bra 	$L__BB0_9;

	shl.b32 	%r19, %r1, 2;
	suld.b.2d.v2.b16.trap {%rs19, %rs20}, [%rd9, {%r19, %r2}];
	// begin inline asm
	{  cvt.f32.f16 %f30, %rs19;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f31, %rs20;}

	// end inline asm
	cvt.rn.f32.u32 	%f34, %r3;
	mul.f32 	%f32, %f30, %f34;
	cvt.rn.f32.u32 	%f35, %r4;
	mul.f32 	%f33, %f31, %f35;
	cvta.to.global.u64 	%rd22, %rd10;
	shl.b64 	%rd23, %rd1, 2;
	add.s64 	%rd24, %rd22, %rd23;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs22, %f33;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs21, %f32;}

	// end inline asm
	st.global.v2.u16 	[%rd24], {%rs21, %rs22};

$L__BB0_9:
	ret;

}
	// .globl	kernelLoadWithAlpha
.visible .entry kernelLoadWithAlpha(
	.param .u32 kernelLoadWithAlpha_param_0,
	.param .u32 kernelLoadWithAlpha_param_1,
	.param .u64 kernelLoadWithAlpha_param_2,
	.param .u64 kernelLoadWithAlpha_param_3,
	.param .u64 kernelLoadWithAlpha_param_4,
	.param .u64 kernelLoadWithAlpha_param_5,
	.param .u64 kernelLoadWithAlpha_param_6,
	.param .u32 kernelLoadWithAlpha_param_7,
	.param .u64 kernelLoadWithAlpha_param_8,
	.param .u64 kernelLoadWithAlpha_param_9,
	.param .u64 kernelLoadWithAlpha_param_10,
	.param .u64 kernelLoadWithAlpha_param_11
)
{
	.reg .pred 	%p<8>;
	.reg .b16 	%rs<23>;
	.reg .f32 	%f<40>;
	.reg .b32 	%r<20>;
	.reg .b64 	%rd<25>;


	ld.param.u32 	%r3, [kernelLoadWithAlpha_param_0];
	ld.param.u32 	%r4, [kernelLoadWithAlpha_param_1];
	ld.param.u64 	%rd2, [kernelLoadWithAlpha_param_2];
	ld.param.u64 	%rd3, [kernelLoadWithAlpha_param_3];
	ld.param.u64 	%rd4, [kernelLoadWithAlpha_param_4];
	ld.param.u64 	%rd5, [kernelLoadWithAlpha_param_5];
	ld.param.u64 	%rd6, [kernelLoadWithAlpha_param_6];
	ld.param.u32 	%r5, [kernelLoadWithAlpha_param_7];
	ld.param.u64 	%rd7, [kernelLoadWithAlpha_param_8];
	ld.param.u64 	%rd8, [kernelLoadWithAlpha_param_9];
	ld.param.u64 	%rd9, [kernelLoadWithAlpha_param_10];
	ld.param.u64 	%rd10, [kernelLoadWithAlpha_param_11];
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r8, %tid.x;
	mad.lo.s32 	%r1, %r7, %r6, %r8;
	mov.u32 	%r9, %ntid.y;
	mov.u32 	%r10, %ctaid.y;
	mov.u32 	%r11, %tid.y;
	mad.lo.s32 	%r2, %r10, %r9, %r11;
	setp.ge.u32 	%p1, %r1, %r3;
	setp.ge.u32 	%p2, %r2, %r4;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB1_9;

	cvta.to.global.u64 	%rd11, %rd3;
	shl.b32 	%r12, %r1, 3;
	suld.b.2d.v4.b16.trap {%rs5, %rs6, %rs7, %rs8}, [%rd2, {%r12, %r2}];
	// begin inline asm
	{  cvt.f32.f16 %f10, %rs5;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f11, %rs6;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f12, %rs7;}

	// end inline asm
	mov.f32 	%f14, 0f461C4000;
	min.f32 	%f15, %f10, %f14;
	mov.f32 	%f16, 0f00000000;
	min.f32 	%f17, %f11, %f14;
	min.f32 	%f18, %f12, %f14;
	mad.lo.s32 	%r13, %r2, %r3, %r1;
	cvt.u64.u32 	%rd1, %r13;
	mul.wide.u32 	%rd12, %r13, 16;
	add.s64 	%rd13, %rd11, %rd12;
	max.f32 	%f19, %f16, %f18;
	max.f32 	%f20, %f16, %f17;
	max.f32 	%f21, %f16, %f15;
	// begin inline asm
	{  cvt.f32.f16 %f13, %rs8;}

	// end inline asm
	st.global.v4.f32 	[%rd13], {%f21, %f20, %f19, %f13};
	setp.eq.s64 	%p4, %rd4, 0;
	@%p4 bra 	$L__BB1_5;

	shl.b32 	%r14, %r1, 2;
	mov.u32 	%r15, 0;
	suld.b.a2d.v4.b8.trap {%rs1, %rs2, %rs3, %rs4}, [%rd4, {%r15, %r14, %r2, %r2}];
	mov.u32 	%r16, 1;
	suld.b.a2d.v4.b8.trap {%rs9, %rs10, %rs11, %rs12}, [%rd4, {%r16, %r14, %r2, %r2}];
	and.b16  	%rs13, %rs9, 255;
	and.b16  	%rs14, %rs10, 255;
	and.b16  	%rs15, %rs11, 255;
	cvt.rn.f32.u16 	%f22, %rs15;
	mul.f32 	%f37, %f22, 0f3B808081;
	cvt.rn.f32.u16 	%f23, %rs14;
	mul.f32 	%f38, %f23, 0f3B808081;
	cvt.rn.f32.u16 	%f24, %rs13;
	mul.f32 	%f39, %f24, 0f3B808081;
	setp.eq.s32 	%p5, %r5, 0;
	@%p5 bra 	$L__BB1_4;

	fma.rn.f32 	%f37, %f37, 0f40000000, 0fBF800000;
	fma.rn.f32 	%f38, %f38, 0f40000000, 0fBF800000;
	fma.rn.f32 	%f39, %f39, 0f40000000, 0fBF800000;

$L__BB1_4:
	cvta.to.global.u64 	%rd14, %rd5;
	mul.lo.s64 	%rd15, %rd1, 12;
	add.s64 	%rd16, %rd14, %rd15;
	and.b16  	%rs16, %rs3, 255;
	cvt.rn.f32.u16 	%f25, %rs16;
	mul.f32 	%f26, %f25, 0f3B808081;
	st.global.f32 	[%rd16], %f26;
	and.b16  	%rs17, %rs2, 255;
	cvt.rn.f32.u16 	%f27, %rs17;
	mul.f32 	%f28, %f27, 0f3B808081;
	st.global.f32 	[%rd16+4], %f28;
	and.b16  	%rs18, %rs1, 255;
	cvt.rn.f32.u16 	%f29, %rs18;
	mul.f32 	%f30, %f29, 0f3B808081;
	st.global.f32 	[%rd16+8], %f30;
	cvta.to.global.u64 	%rd17, %rd6;
	add.s64 	%rd18, %rd17, %rd15;
	st.global.f32 	[%rd18], %f37;
	st.global.f32 	[%rd18+4], %f38;
	st.global.f32 	[%rd18+8], %f39;

$L__BB1_5:
	setp.eq.s64 	%p6, %rd7, 0;
	@%p6 bra 	$L__BB1_7;

	shl.b32 	%r17, %r1, 2;
	suld.b.2d.b32.trap {%r18}, [%rd7, {%r17, %r2}];
	cvta.to.global.u64 	%rd19, %rd8;
	shl.b64 	%rd20, %rd1, 2;
	add.s64 	%rd21, %rd19, %rd20;
	st.global.u32 	[%rd21], %r18;

$L__BB1_7:
	setp.eq.s64 	%p7, %rd9, 0;
	@%p7 bra 	$L__BB1_9;

	shl.b32 	%r19, %r1, 2;
	suld.b.2d.v2.b16.trap {%rs19, %rs20}, [%rd9, {%r19, %r2}];
	// begin inline asm
	{  cvt.f32.f16 %f31, %rs19;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f32, %rs20;}

	// end inline asm
	cvt.rn.f32.u32 	%f35, %r3;
	mul.f32 	%f33, %f31, %f35;
	cvt.rn.f32.u32 	%f36, %r4;
	mul.f32 	%f34, %f32, %f36;
	cvta.to.global.u64 	%rd22, %rd10;
	shl.b64 	%rd23, %rd1, 2;
	add.s64 	%rd24, %rd22, %rd23;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs22, %f34;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs21, %f33;}

	// end inline asm
	st.global.v2.u16 	[%rd24], {%rs21, %rs22};

$L__BB1_9:
	ret;

}
	// .globl	kernelLoadAccumulate
.visible .entry kernelLoadAccumulate(
	.param .u32 kernelLoadAccumulate_param_0,
	.param .u32 kernelLoadAccumulate_param_1,
	.param .u64 kernelLoadAccumulate_param_2,
	.param .u64 kernelLoadAccumulate_param_3,
	.param .u64 kernelLoadAccumulate_param_4,
	.param .u64 kernelLoadAccumulate_param_5,
	.param .u64 kernelLoadAccumulate_param_6,
	.param .u32 kernelLoadAccumulate_param_7,
	.param .u64 kernelLoadAccumulate_param_8,
	.param .u64 kernelLoadAccumulate_param_9,
	.param .f32 kernelLoadAccumulate_param_10
)
{
	.reg .pred 	%p<7>;
	.reg .b16 	%rs<19>;
	.reg .f32 	%f<66>;
	.reg .b32 	%r<19>;
	.reg .b64 	%rd<20>;


	ld.param.u32 	%r3, [kernelLoadAccumulate_param_0];
	ld.param.u32 	%r5, [kernelLoadAccumulate_param_1];
	ld.param.u64 	%rd4, [kernelLoadAccumulate_param_2];
	ld.param.u64 	%rd5, [kernelLoadAccumulate_param_3];
	ld.param.u64 	%rd6, [kernelLoadAccumulate_param_4];
	ld.param.u64 	%rd7, [kernelLoadAccumulate_param_5];
	ld.param.u64 	%rd8, [kernelLoadAccumulate_param_6];
	ld.param.u32 	%r4, [kernelLoadAccumulate_param_7];
	ld.param.u64 	%rd9, [kernelLoadAccumulate_param_8];
	ld.param.u64 	%rd10, [kernelLoadAccumulate_param_9];
	ld.param.f32 	%f17, [kernelLoadAccumulate_param_10];
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r8, %tid.x;
	mad.lo.s32 	%r1, %r7, %r6, %r8;
	mov.u32 	%r9, %ntid.y;
	mov.u32 	%r10, %ctaid.y;
	mov.u32 	%r11, %tid.y;
	mad.lo.s32 	%r2, %r10, %r9, %r11;
	setp.ge.u32 	%p1, %r1, %r3;
	setp.ge.u32 	%p2, %r2, %r5;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB2_7;

	shl.b32 	%r12, %r1, 3;
	suld.b.2d.v4.b16.trap {%rs5, %rs6, %rs7, %rs8}, [%rd4, {%r12, %r2}];
	// begin inline asm
	{  cvt.f32.f16 %f18, %rs5;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f19, %rs6;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f20, %rs7;}

	// end inline asm
	mad.lo.s32 	%r13, %r2, %r3, %r1;
	cvt.u64.u32 	%rd1, %r13;
	cvta.to.global.u64 	%rd11, %rd5;
	mul.wide.u32 	%rd12, %r13, 12;
	add.s64 	%rd13, %rd11, %rd12;
	mov.f32 	%f21, 0f461C4000;
	min.f32 	%f22, %f18, %f21;
	mov.f32 	%f23, 0f00000000;
	max.f32 	%f24, %f23, %f22;
	min.f32 	%f25, %f19, %f21;
	max.f32 	%f26, %f23, %f25;
	min.f32 	%f27, %f20, %f21;
	max.f32 	%f28, %f23, %f27;
	mov.f32 	%f29, 0f3F800000;
	sub.f32 	%f1, %f29, %f17;
	ld.global.f32 	%f30, [%rd13];
	mul.f32 	%f31, %f24, %f17;
	fma.rn.f32 	%f32, %f1, %f30, %f31;
	ld.global.f32 	%f33, [%rd13+4];
	mul.f32 	%f34, %f26, %f17;
	fma.rn.f32 	%f35, %f1, %f33, %f34;
	ld.global.f32 	%f36, [%rd13+8];
	mul.f32 	%f37, %f28, %f17;
	fma.rn.f32 	%f38, %f1, %f36, %f37;
	st.global.f32 	[%rd13], %f32;
	st.global.f32 	[%rd13+4], %f35;
	st.global.f32 	[%rd13+8], %f38;
	setp.eq.s64 	%p4, %rd6, 0;
	@%p4 bra 	$L__BB2_5;

	shl.b32 	%r14, %r1, 2;
	mov.u32 	%r15, 0;
	suld.b.a2d.v4.b8.trap {%rs1, %rs2, %rs3, %rs4}, [%rd6, {%r15, %r14, %r2, %r2}];
	mov.u32 	%r16, 1;
	suld.b.a2d.v4.b8.trap {%rs9, %rs10, %rs11, %rs12}, [%rd6, {%r16, %r14, %r2, %r2}];
	and.b16  	%rs13, %rs9, 255;
	and.b16  	%rs14, %rs10, 255;
	and.b16  	%rs15, %rs11, 255;
	cvt.rn.f32.u16 	%f39, %rs15;
	mul.f32 	%f65, %f39, 0f3B808081;
	cvt.rn.f32.u16 	%f40, %rs14;
	mul.f32 	%f64, %f40, 0f3B808081;
	cvt.rn.f32.u16 	%f41, %rs13;
	mul.f32 	%f63, %f41, 0f3B808081;
	cvta.to.global.u64 	%rd14, %rd7;
	mul.lo.s64 	%rd15, %rd1, 12;
	add.s64 	%rd2, %rd14, %rd15;
	ld.global.f32 	%f5, [%rd2];
	ld.global.f32 	%f6, [%rd2+4];
	ld.global.f32 	%f7, [%rd2+8];
	cvta.to.global.u64 	%rd16, %rd8;
	add.s64 	%rd3, %rd16, %rd15;
	ld.global.f32 	%f8, [%rd3];
	ld.global.f32 	%f9, [%rd3+4];
	ld.global.f32 	%f10, [%rd3+8];
	setp.eq.s32 	%p5, %r4, 0;
	@%p5 bra 	$L__BB2_4;

	fma.rn.f32 	%f65, %f65, 0f40000000, 0fBF800000;
	fma.rn.f32 	%f64, %f64, 0f40000000, 0fBF800000;
	fma.rn.f32 	%f63, %f63, 0f40000000, 0fBF800000;

$L__BB2_4:
	and.b16  	%rs16, %rs3, 255;
	cvt.rn.f32.u16 	%f42, %rs16;
	mul.f32 	%f43, %f42, 0f3B808081;
	mul.f32 	%f44, %f43, %f17;
	fma.rn.f32 	%f45, %f1, %f5, %f44;
	and.b16  	%rs17, %rs2, 255;
	cvt.rn.f32.u16 	%f46, %rs17;
	mul.f32 	%f47, %f46, 0f3B808081;
	mul.f32 	%f48, %f47, %f17;
	fma.rn.f32 	%f49, %f1, %f6, %f48;
	and.b16  	%rs18, %rs1, 255;
	cvt.rn.f32.u16 	%f50, %rs18;
	mul.f32 	%f51, %f50, 0f3B808081;
	mul.f32 	%f52, %f51, %f17;
	fma.rn.f32 	%f53, %f1, %f7, %f52;
	st.global.f32 	[%rd2], %f45;
	st.global.f32 	[%rd2+4], %f49;
	st.global.f32 	[%rd2+8], %f53;
	mul.f32 	%f54, %f65, %f17;
	fma.rn.f32 	%f55, %f1, %f8, %f54;
	mul.f32 	%f56, %f64, %f17;
	fma.rn.f32 	%f57, %f1, %f9, %f56;
	mul.f32 	%f58, %f63, %f17;
	fma.rn.f32 	%f59, %f1, %f10, %f58;
	st.global.f32 	[%rd3], %f55;
	st.global.f32 	[%rd3+4], %f57;
	st.global.f32 	[%rd3+8], %f59;

$L__BB2_5:
	setp.eq.s64 	%p6, %rd9, 0;
	@%p6 bra 	$L__BB2_7;

	shl.b32 	%r17, %r1, 2;
	suld.b.2d.b32.trap {%r18}, [%rd9, {%r17, %r2}];
	mov.b32 	%f60, %r18;
	cvta.to.global.u64 	%rd17, %rd10;
	shl.b64 	%rd18, %rd1, 2;
	add.s64 	%rd19, %rd17, %rd18;
	ld.global.f32 	%f61, [%rd19];
	max.f32 	%f62, %f61, %f60;
	st.global.f32 	[%rd19], %f62;

$L__BB2_7:
	ret;

}
	// .globl	kernelLoadAccumulateWithAlpha
.visible .entry kernelLoadAccumulateWithAlpha(
	.param .u32 kernelLoadAccumulateWithAlpha_param_0,
	.param .u32 kernelLoadAccumulateWithAlpha_param_1,
	.param .u64 kernelLoadAccumulateWithAlpha_param_2,
	.param .u64 kernelLoadAccumulateWithAlpha_param_3,
	.param .u64 kernelLoadAccumulateWithAlpha_param_4,
	.param .u64 kernelLoadAccumulateWithAlpha_param_5,
	.param .u64 kernelLoadAccumulateWithAlpha_param_6,
	.param .u32 kernelLoadAccumulateWithAlpha_param_7,
	.param .u64 kernelLoadAccumulateWithAlpha_param_8,
	.param .u64 kernelLoadAccumulateWithAlpha_param_9,
	.param .f32 kernelLoadAccumulateWithAlpha_param_10
)
{
	.reg .pred 	%p<7>;
	.reg .b16 	%rs<19>;
	.reg .f32 	%f<74>;
	.reg .b32 	%r<19>;
	.reg .b64 	%rd<20>;


	ld.param.u32 	%r3, [kernelLoadAccumulateWithAlpha_param_0];
	ld.param.u32 	%r5, [kernelLoadAccumulateWithAlpha_param_1];
	ld.param.u64 	%rd4, [kernelLoadAccumulateWithAlpha_param_2];
	ld.param.u64 	%rd5, [kernelLoadAccumulateWithAlpha_param_3];
	ld.param.u64 	%rd6, [kernelLoadAccumulateWithAlpha_param_4];
	ld.param.u64 	%rd7, [kernelLoadAccumulateWithAlpha_param_5];
	ld.param.u64 	%rd8, [kernelLoadAccumulateWithAlpha_param_6];
	ld.param.u32 	%r4, [kernelLoadAccumulateWithAlpha_param_7];
	ld.param.u64 	%rd9, [kernelLoadAccumulateWithAlpha_param_8];
	ld.param.u64 	%rd10, [kernelLoadAccumulateWithAlpha_param_9];
	ld.param.f32 	%f17, [kernelLoadAccumulateWithAlpha_param_10];
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r8, %tid.x;
	mad.lo.s32 	%r1, %r7, %r6, %r8;
	mov.u32 	%r9, %ntid.y;
	mov.u32 	%r10, %ctaid.y;
	mov.u32 	%r11, %tid.y;
	mad.lo.s32 	%r2, %r10, %r9, %r11;
	setp.ge.u32 	%p1, %r1, %r3;
	setp.ge.u32 	%p2, %r2, %r5;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB3_7;

	cvta.to.global.u64 	%rd11, %rd5;
	shl.b32 	%r12, %r1, 3;
	suld.b.2d.v4.b16.trap {%rs5, %rs6, %rs7, %rs8}, [%rd4, {%r12, %r2}];
	// begin inline asm
	{  cvt.f32.f16 %f18, %rs5;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f19, %rs6;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f20, %rs7;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f21, %rs8;}

	// end inline asm
	mad.lo.s32 	%r13, %r2, %r3, %r1;
	cvt.u64.u32 	%rd1, %r13;
	mul.wide.u32 	%rd12, %r13, 16;
	add.s64 	%rd13, %rd11, %rd12;
	ld.global.v4.f32 	{%f22, %f23, %f24, %f25}, [%rd13];
	mov.f32 	%f30, 0f461C4000;
	min.f32 	%f31, %f18, %f30;
	mov.f32 	%f32, 0f00000000;
	max.f32 	%f33, %f32, %f31;
	min.f32 	%f34, %f19, %f30;
	max.f32 	%f35, %f32, %f34;
	min.f32 	%f36, %f20, %f30;
	max.f32 	%f37, %f32, %f36;
	mov.f32 	%f38, 0f3F800000;
	sub.f32 	%f1, %f38, %f17;
	mul.f32 	%f39, %f33, %f17;
	mul.f32 	%f40, %f35, %f17;
	mul.f32 	%f41, %f37, %f17;
	mul.f32 	%f42, %f1, %f25;
	fma.rn.f32 	%f43, %f21, %f17, %f42;
	fma.rn.f32 	%f44, %f1, %f24, %f41;
	fma.rn.f32 	%f45, %f1, %f23, %f40;
	fma.rn.f32 	%f46, %f1, %f22, %f39;
	st.global.v4.f32 	[%rd13], {%f46, %f45, %f44, %f43};
	setp.eq.s64 	%p4, %rd6, 0;
	@%p4 bra 	$L__BB3_5;

	shl.b32 	%r14, %r1, 2;
	mov.u32 	%r15, 0;
	suld.b.a2d.v4.b8.trap {%rs1, %rs2, %rs3, %rs4}, [%rd6, {%r15, %r14, %r2, %r2}];
	mov.u32 	%r16, 1;
	suld.b.a2d.v4.b8.trap {%rs9, %rs10, %rs11, %rs12}, [%rd6, {%r16, %r14, %r2, %r2}];
	and.b16  	%rs13, %rs9, 255;
	and.b16  	%rs14, %rs10, 255;
	and.b16  	%rs15, %rs11, 255;
	cvt.rn.f32.u16 	%f47, %rs15;
	mul.f32 	%f73, %f47, 0f3B808081;
	cvt.rn.f32.u16 	%f48, %rs14;
	mul.f32 	%f72, %f48, 0f3B808081;
	cvt.rn.f32.u16 	%f49, %rs13;
	mul.f32 	%f71, %f49, 0f3B808081;
	cvta.to.global.u64 	%rd14, %rd7;
	mul.lo.s64 	%rd15, %rd1, 12;
	add.s64 	%rd2, %rd14, %rd15;
	ld.global.f32 	%f5, [%rd2];
	ld.global.f32 	%f6, [%rd2+4];
	ld.global.f32 	%f7, [%rd2+8];
	cvta.to.global.u64 	%rd16, %rd8;
	add.s64 	%rd3, %rd16, %rd15;
	ld.global.f32 	%f8, [%rd3];
	ld.global.f32 	%f9, [%rd3+4];
	ld.global.f32 	%f10, [%rd3+8];
	setp.eq.s32 	%p5, %r4, 0;
	@%p5 bra 	$L__BB3_4;

	fma.rn.f32 	%f73, %f73, 0f40000000, 0fBF800000;
	fma.rn.f32 	%f72, %f72, 0f40000000, 0fBF800000;
	fma.rn.f32 	%f71, %f71, 0f40000000, 0fBF800000;

$L__BB3_4:
	and.b16  	%rs16, %rs3, 255;
	cvt.rn.f32.u16 	%f50, %rs16;
	mul.f32 	%f51, %f50, 0f3B808081;
	mul.f32 	%f52, %f51, %f17;
	fma.rn.f32 	%f53, %f1, %f5, %f52;
	and.b16  	%rs17, %rs2, 255;
	cvt.rn.f32.u16 	%f54, %rs17;
	mul.f32 	%f55, %f54, 0f3B808081;
	mul.f32 	%f56, %f55, %f17;
	fma.rn.f32 	%f57, %f1, %f6, %f56;
	and.b16  	%rs18, %rs1, 255;
	cvt.rn.f32.u16 	%f58, %rs18;
	mul.f32 	%f59, %f58, 0f3B808081;
	mul.f32 	%f60, %f1, %f7;
	fma.rn.f32 	%f61, %f59, %f17, %f60;
	st.global.f32 	[%rd2], %f53;
	st.global.f32 	[%rd2+4], %f57;
	st.global.f32 	[%rd2+8], %f61;
	mul.f32 	%f62, %f73, %f17;
	fma.rn.f32 	%f63, %f1, %f8, %f62;
	mul.f32 	%f64, %f72, %f17;
	fma.rn.f32 	%f65, %f1, %f9, %f64;
	mul.f32 	%f66, %f71, %f17;
	fma.rn.f32 	%f67, %f1, %f10, %f66;
	st.global.f32 	[%rd3], %f63;
	st.global.f32 	[%rd3+4], %f65;
	st.global.f32 	[%rd3+8], %f67;

$L__BB3_5:
	setp.eq.s64 	%p6, %rd9, 0;
	@%p6 bra 	$L__BB3_7;

	shl.b32 	%r17, %r1, 2;
	suld.b.2d.b32.trap {%r18}, [%rd9, {%r17, %r2}];
	mov.b32 	%f68, %r18;
	cvta.to.global.u64 	%rd17, %rd10;
	shl.b64 	%rd18, %rd1, 2;
	add.s64 	%rd19, %rd17, %rd18;
	ld.global.f32 	%f69, [%rd19];
	max.f32 	%f70, %f69, %f68;
	st.global.f32 	[%rd19], %f70;

$L__BB3_7:
	ret;

}
	// .globl	kernelStore
.visible .entry kernelStore(
	.param .u32 kernelStore_param_0,
	.param .u32 kernelStore_param_1,
	.param .u64 kernelStore_param_2,
	.param .u64 kernelStore_param_3,
	.param .u64 kernelStore_param_4,
	.param .f32 kernelStore_param_5,
	.param .u64 kernelStore_param_6,
	.param .f32 kernelStore_param_7
)
{
	.reg .pred 	%p<7>;
	.reg .b16 	%rs<9>;
	.reg .f32 	%f<47>;
	.reg .b32 	%r<14>;
	.reg .b64 	%rd<15>;


	ld.param.u32 	%r3, [kernelStore_param_0];
	ld.param.u32 	%r4, [kernelStore_param_1];
	ld.param.u64 	%rd2, [kernelStore_param_2];
	ld.param.u64 	%rd3, [kernelStore_param_3];
	ld.param.u64 	%rd4, [kernelStore_param_4];
	ld.param.f32 	%f13, [kernelStore_param_5];
	ld.param.u64 	%rd5, [kernelStore_param_6];
	ld.param.f32 	%f14, [kernelStore_param_7];
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %ctaid.x;
	mov.u32 	%r7, %tid.x;
	mad.lo.s32 	%r1, %r6, %r5, %r7;
	mov.u32 	%r8, %ntid.y;
	mov.u32 	%r9, %ctaid.y;
	mov.u32 	%r10, %tid.y;
	mad.lo.s32 	%r2, %r9, %r8, %r10;
	setp.ge.u32 	%p1, %r1, %r3;
	setp.ge.u32 	%p2, %r2, %r4;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB4_6;

	mad.lo.s32 	%r11, %r2, %r3, %r1;
	cvt.u64.u32 	%rd1, %r11;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r11, 12;
	add.s64 	%rd8, %rd6, %rd7;
	ld.global.f32 	%f1, [%rd8];
	ld.global.f32 	%f2, [%rd8+4];
	ld.global.f32 	%f3, [%rd8+8];
	setp.gt.f32 	%p4, %f14, 0f00000000;
	@%p4 bra 	$L__BB4_3;
	bra.uni 	$L__BB4_2;

$L__BB4_3:
	cvta.to.global.u64 	%rd9, %rd3;
	mul.lo.s64 	%rd10, %rd1, 12;
	add.s64 	%rd11, %rd9, %rd10;
	ld.global.f32 	%f46, [%rd11];
	ld.global.f32 	%f45, [%rd11+4];
	ld.global.f32 	%f44, [%rd11+8];
	setp.leu.f32 	%p5, %f13, 0f00000000;
	@%p5 bra 	$L__BB4_5;

	cvta.to.global.u64 	%rd12, %rd4;
	add.s64 	%rd14, %rd12, %rd10;
	mov.f32 	%f18, 0f3F800000;
	sub.f32 	%f19, %f18, %f13;
	ld.global.f32 	%f20, [%rd14];
	mul.f32 	%f21, %f20, %f13;
	fma.rn.f32 	%f46, %f19, %f46, %f21;
	ld.global.f32 	%f22, [%rd14+4];
	mul.f32 	%f23, %f22, %f13;
	fma.rn.f32 	%f45, %f19, %f45, %f23;
	ld.global.f32 	%f24, [%rd14+8];
	mul.f32 	%f25, %f24, %f13;
	fma.rn.f32 	%f44, %f19, %f44, %f25;

$L__BB4_5:
	mul.f32 	%f29, %f2, 0f3F1645A2;
	fma.rn.f32 	%f30, %f1, 0f3E991687, %f29;
	fma.rn.f32 	%f31, %f3, 0f3DE978D5, %f30;
	cvt.sat.f32.f32 	%f32, %f31;
	mul.f32 	%f33, %f46, 0f3E991687;
	fma.rn.f32 	%f34, %f45, 0f3F1645A2, %f33;
	fma.rn.f32 	%f35, %f44, 0f3DE978D5, %f34;
	cvt.sat.f32.f32 	%f36, %f35;
	add.f32 	%f37, %f36, 0f3DCCCCCD;
	setp.gt.f32 	%p6, %f32, %f37;
	selp.f32 	%f38, 0f3F800000, %f14, %p6;
	mov.f32 	%f39, 0f3F800000;
	sub.f32 	%f40, %f39, %f38;
	mul.f32 	%f41, %f1, %f40;
	fma.rn.f32 	%f26, %f46, %f38, %f41;
	mul.f32 	%f42, %f2, %f40;
	fma.rn.f32 	%f27, %f45, %f38, %f42;
	mul.f32 	%f43, %f3, %f40;
	fma.rn.f32 	%f28, %f44, %f38, %f43;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs5, %f26;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs6, %f27;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs7, %f28;}

	// end inline asm
	shl.b32 	%r13, %r1, 3;
	mov.u16 	%rs8, 0;
	sust.b.2d.v4.b16.trap 	[%rd5, {%r13, %r2}], {%rs5, %rs6, %rs7, %rs8};
	bra.uni 	$L__BB4_6;

$L__BB4_2:
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f1;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs2, %f2;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs3, %f3;}

	// end inline asm
	shl.b32 	%r12, %r1, 3;
	mov.u16 	%rs4, 0;
	sust.b.2d.v4.b16.trap 	[%rd5, {%r12, %r2}], {%rs1, %rs2, %rs3, %rs4};

$L__BB4_6:
	ret;

}
	// .globl	kernelStoreWithAlpha
.visible .entry kernelStoreWithAlpha(
	.param .u32 kernelStoreWithAlpha_param_0,
	.param .u32 kernelStoreWithAlpha_param_1,
	.param .u64 kernelStoreWithAlpha_param_2,
	.param .u64 kernelStoreWithAlpha_param_3,
	.param .u64 kernelStoreWithAlpha_param_4,
	.param .f32 kernelStoreWithAlpha_param_5,
	.param .u64 kernelStoreWithAlpha_param_6,
	.param .f32 kernelStoreWithAlpha_param_7
)
{
	.reg .pred 	%p<7>;
	.reg .b16 	%rs<9>;
	.reg .f32 	%f<62>;
	.reg .b32 	%r<14>;
	.reg .b64 	%rd<15>;


	ld.param.u32 	%r3, [kernelStoreWithAlpha_param_0];
	ld.param.u32 	%r4, [kernelStoreWithAlpha_param_1];
	ld.param.u64 	%rd2, [kernelStoreWithAlpha_param_2];
	ld.param.u64 	%rd3, [kernelStoreWithAlpha_param_3];
	ld.param.u64 	%rd4, [kernelStoreWithAlpha_param_4];
	ld.param.f32 	%f14, [kernelStoreWithAlpha_param_5];
	ld.param.u64 	%rd5, [kernelStoreWithAlpha_param_6];
	ld.param.f32 	%f15, [kernelStoreWithAlpha_param_7];
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %ctaid.x;
	mov.u32 	%r7, %tid.x;
	mad.lo.s32 	%r1, %r6, %r5, %r7;
	mov.u32 	%r8, %ntid.y;
	mov.u32 	%r9, %ctaid.y;
	mov.u32 	%r10, %tid.y;
	mad.lo.s32 	%r2, %r9, %r8, %r10;
	setp.ge.u32 	%p1, %r1, %r3;
	setp.ge.u32 	%p2, %r2, %r4;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB5_6;

	cvta.to.global.u64 	%rd6, %rd2;
	mad.lo.s32 	%r11, %r2, %r3, %r1;
	cvt.u64.u32 	%rd1, %r11;
	mul.wide.u32 	%rd7, %r11, 16;
	add.s64 	%rd8, %rd6, %rd7;
	ld.global.v4.f32 	{%f16, %f17, %f18, %f19}, [%rd8];
	setp.gt.f32 	%p4, %f15, 0f00000000;
	@%p4 bra 	$L__BB5_3;
	bra.uni 	$L__BB5_2;

$L__BB5_3:
	cvta.to.global.u64 	%rd9, %rd3;
	shl.b64 	%rd10, %rd1, 4;
	add.s64 	%rd11, %rd9, %rd10;
	ld.global.v4.f32 	{%f61, %f60, %f59, %f27}, [%rd11];
	setp.leu.f32 	%p5, %f14, 0f00000000;
	@%p5 bra 	$L__BB5_5;

	cvta.to.global.u64 	%rd12, %rd4;
	add.s64 	%rd14, %rd12, %rd10;
	ld.global.v4.f32 	{%f28, %f29, %f30, %f31}, [%rd14];
	mov.f32 	%f35, 0f3F800000;
	sub.f32 	%f36, %f35, %f14;
	mul.f32 	%f37, %f28, %f14;
	fma.rn.f32 	%f61, %f36, %f61, %f37;
	mul.f32 	%f38, %f29, %f14;
	fma.rn.f32 	%f60, %f36, %f60, %f38;
	mul.f32 	%f39, %f30, %f14;
	fma.rn.f32 	%f59, %f36, %f59, %f39;

$L__BB5_5:
	mul.f32 	%f44, %f17, 0f3F1645A2;
	fma.rn.f32 	%f45, %f16, 0f3E991687, %f44;
	fma.rn.f32 	%f46, %f18, 0f3DE978D5, %f45;
	cvt.sat.f32.f32 	%f47, %f46;
	mul.f32 	%f48, %f61, 0f3E991687;
	fma.rn.f32 	%f49, %f60, 0f3F1645A2, %f48;
	fma.rn.f32 	%f50, %f59, 0f3DE978D5, %f49;
	cvt.sat.f32.f32 	%f51, %f50;
	add.f32 	%f52, %f51, 0f3DCCCCCD;
	setp.gt.f32 	%p6, %f47, %f52;
	selp.f32 	%f53, 0f3F800000, %f15, %p6;
	mov.f32 	%f54, 0f3F800000;
	sub.f32 	%f55, %f54, %f53;
	mul.f32 	%f56, %f16, %f55;
	fma.rn.f32 	%f40, %f61, %f53, %f56;
	mul.f32 	%f57, %f17, %f55;
	fma.rn.f32 	%f41, %f60, %f53, %f57;
	mul.f32 	%f58, %f18, %f55;
	fma.rn.f32 	%f42, %f59, %f53, %f58;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs5, %f40;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs6, %f41;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs7, %f42;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs8, %f19;}

	// end inline asm
	shl.b32 	%r13, %r1, 3;
	sust.b.2d.v4.b16.trap 	[%rd5, {%r13, %r2}], {%rs5, %rs6, %rs7, %rs8};
	bra.uni 	$L__BB5_6;

$L__BB5_2:
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f16;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs2, %f17;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs3, %f18;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs4, %f19;}

	// end inline asm
	shl.b32 	%r12, %r1, 3;
	sust.b.2d.v4.b16.trap 	[%rd5, {%r12, %r2}], {%rs1, %rs2, %rs3, %rs4};

$L__BB5_6:
	ret;

}
	// .globl	kernelStoreDepth
.visible .entry kernelStoreDepth(
	.param .u32 kernelStoreDepth_param_0,
	.param .u32 kernelStoreDepth_param_1,
	.param .u64 kernelStoreDepth_param_2,
	.param .u64 kernelStoreDepth_param_3
)
{
	.reg .pred 	%p<4>;
	.reg .f32 	%f<2>;
	.reg .b32 	%r<14>;
	.reg .b64 	%rd<8>;


	ld.param.u32 	%r3, [kernelStoreDepth_param_0];
	ld.param.u32 	%r4, [kernelStoreDepth_param_1];
	ld.param.u64 	%rd1, [kernelStoreDepth_param_2];
	ld.param.u64 	%rd2, [kernelStoreDepth_param_3];
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %tid.x;
	mad.lo.s32 	%r1, %r5, %r6, %r7;
	mov.u32 	%r8, %ntid.y;
	mov.u32 	%r9, %ctaid.y;
	mov.u32 	%r10, %tid.y;
	mad.lo.s32 	%r2, %r9, %r8, %r10;
	setp.ge.u32 	%p1, %r1, %r3;
	setp.ge.u32 	%p2, %r2, %r4;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB6_2;

	mul.lo.s32 	%r11, %r2, %r3;
	cvt.u64.u32 	%rd4, %r11;
	cvt.u64.u32 	%rd5, %r1;
	add.s64 	%rd6, %rd4, %rd5;
	shl.b64 	%rd7, %rd6, 2;
	add.s64 	%rd3, %rd2, %rd7;
	// begin inline asm
	ld.global.nc.f32 %f1, [%rd3];
	// end inline asm
	mov.b32 	%r12, %f1;
	shl.b32 	%r13, %r1, 2;
	sust.b.2d.b32.trap 	[%rd1, {%r13, %r2}], {%r12};

$L__BB6_2:
	ret;

}
	// .globl	kernelRemap
.visible .entry kernelRemap(
	.param .u32 kernelRemap_param_0,
	.param .u32 kernelRemap_param_1,
	.param .u64 kernelRemap_param_2
)
{
	.reg .pred 	%p<4>;
	.reg .b16 	%rs<7>;
	.reg .f32 	%f<7>;
	.reg .b32 	%r<12>;
	.reg .b64 	%rd<5>;


	ld.param.u32 	%r3, [kernelRemap_param_0];
	ld.param.u32 	%r4, [kernelRemap_param_1];
	ld.param.u64 	%rd1, [kernelRemap_param_2];
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %ctaid.x;
	mov.u32 	%r7, %tid.x;
	mad.lo.s32 	%r1, %r6, %r5, %r7;
	mov.u32 	%r8, %ntid.y;
	mov.u32 	%r9, %ctaid.y;
	mov.u32 	%r10, %tid.y;
	mad.lo.s32 	%r2, %r9, %r8, %r10;
	setp.ge.u32 	%p1, %r1, %r3;
	setp.ge.u32 	%p2, %r2, %r4;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB7_2;

	mad.lo.s32 	%r11, %r2, %r3, %r1;
	cvta.to.global.u64 	%rd2, %rd1;
	mul.wide.u32 	%rd3, %r11, 6;
	add.s64 	%rd4, %rd2, %rd3;
	ld.global.u16 	%rs1, [%rd4];
	ld.global.u16 	%rs3, [%rd4+2];
	ld.global.u16 	%rs5, [%rd4+4];
	// begin inline asm
	{  cvt.f32.f16 %f1, %rs1;}

	// end inline asm
	fma.rn.f32 	%f2, %f1, 0f40000000, 0fBF800000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs2, %f2;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f3, %rs3;}

	// end inline asm
	fma.rn.f32 	%f4, %f3, 0f40000000, 0fBF800000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs4, %f4;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f5, %rs5;}

	// end inline asm
	fma.rn.f32 	%f6, %f5, 0f40000000, 0fBF800000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs6, %f6;}

	// end inline asm
	st.global.u16 	[%rd4], %rs2;
	st.global.u16 	[%rd4+2], %rs4;
	st.global.u16 	[%rd4+4], %rs6;

$L__BB7_2:
	ret;

}

