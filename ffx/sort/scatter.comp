#include "params.sh"
#include "buffers.sh"
#include "scanPrefixBlock.comp"

USE_KEYBUFFER(bSrcBuffer);
USE_BUFFER(uint,bSumTable);
USE_LOADSTORE_KEYBUFFER(bDstBuffer,0);
#ifdef kRS_ValueCopy
	USE_VALUEBUFFER(bSrcPayload);
	USE_LOADSTORE_VALUEBUFFER(bDstPayload,1);
#endif

#define SrcBuffer	bSrcBuffer
#define SumTable	bSumTable
#define DstBuffer	bDstBuffer
#define SrcPayload	bSrcPayload
#define DstPayload	bDstPayload

// Offset cache to avoid loading the offsets all the time
groupshared uint gs_FFX_PARALLELSORT_BinOffsetCache[FFX_PARALLELSORT_THREADGROUP_SIZE];
// Local histogram for offset calculations
groupshared interlocked_uint gs_FFX_PARALLELSORT_LocalHistogram[FFX_PARALLELSORT_SORT_BIN_COUNT];
// Scratch area for algorithm
groupshared uint gs_FFX_PARALLELSORT_LDSScratch[FFX_PARALLELSORT_THREADGROUP_SIZE];
void FFX_ParallelSort_Scatter_uint(uint localID, uint groupID, uint laneID, uint waveLaneCount, FFX_ParallelSortCB CBuffer, uint ShiftBit)
{
	// Load the sort bin threadgroup offsets into LDS for faster referencing
	if (localID < FFX_PARALLELSORT_SORT_BIN_COUNT)
		gs_FFX_PARALLELSORT_BinOffsetCache[localID] = SumTable[localID * CBuffer.NumThreadGroups + groupID];

	// Wait for everyone to catch up
	groupMemoryBarrierWithGroupSync();

	// Data is processed in blocks, and how many we process can changed based on how much data we are processing
	// versus how many thread groups we are processing with
	int BlockSize = FFX_PARALLELSORT_ELEMENTS_PER_THREAD * FFX_PARALLELSORT_THREADGROUP_SIZE;

	// Figure out this thread group's index into the block data (taking into account thread groups that need to do extra reads)
	uint ThreadgroupBlockStart = (BlockSize * CBuffer.NumBlocksPerThreadGroup * groupID);
	uint NumBlocksToProcess = CBuffer.NumBlocksPerThreadGroup;

	if (groupID >= CBuffer.NumThreadGroups - CBuffer.NumThreadGroupsWithAdditionalBlocks)
	{
		ThreadgroupBlockStart += (groupID - (CBuffer.NumThreadGroups - CBuffer.NumThreadGroupsWithAdditionalBlocks)) * BlockSize;
		NumBlocksToProcess++;
	}

	// Get the block start index for this thread
	uint BlockIndex = ThreadgroupBlockStart + localID;

	// Count value occurences
	uint newCount;
	for (uint BlockCount = 0; BlockCount < NumBlocksToProcess; BlockCount++, BlockIndex += BlockSize)
	{
		uint DataIndex = BlockIndex;
		
		// Pre-load the key values in order to hide some of the read latency
		uint srcKeys[FFX_PARALLELSORT_ELEMENTS_PER_THREAD];
		srcKeys[0] = SrcBuffer[DataIndex];
		srcKeys[1] = SrcBuffer[DataIndex + FFX_PARALLELSORT_THREADGROUP_SIZE];
		srcKeys[2] = SrcBuffer[DataIndex + (FFX_PARALLELSORT_THREADGROUP_SIZE * 2)];
		srcKeys[3] = SrcBuffer[DataIndex + (FFX_PARALLELSORT_THREADGROUP_SIZE * 3)];

#ifdef kRS_ValueCopy
		uint srcValues[FFX_PARALLELSORT_ELEMENTS_PER_THREAD];
		srcValues[0] = SrcPayload[DataIndex];
		srcValues[1] = SrcPayload[DataIndex + FFX_PARALLELSORT_THREADGROUP_SIZE];
		srcValues[2] = SrcPayload[DataIndex + (FFX_PARALLELSORT_THREADGROUP_SIZE * 2)];
		srcValues[3] = SrcPayload[DataIndex + (FFX_PARALLELSORT_THREADGROUP_SIZE * 3)];
#endif // kRS_ValueCopy

		for (int i = 0; i < FFX_PARALLELSORT_ELEMENTS_PER_THREAD; i++)
		{
			// Clear the local histogram
			if (localID < FFX_PARALLELSORT_SORT_BIN_COUNT)
				interlockedStoreGS(gs_FFX_PARALLELSORT_LocalHistogram[localID], 0);

			uint localKey = (DataIndex < CBuffer.NumKeys ? srcKeys[i] : 0xffffffff);
#ifdef kRS_ValueCopy
			uint localValue = (DataIndex < CBuffer.NumKeys ? srcValues[i] : 0);
#endif // kRS_ValueCopy

			// Sort the keys locally in LDS
			for (uint bitShift = 0; bitShift < FFX_PARALLELSORT_SORT_BITS_PER_PASS; bitShift += 2)
			{
				// Figure out the keyIndex
				uint keyIndex = (localKey >> ShiftBit) & 0xf;
				uint bitKey = (keyIndex >> bitShift) & 0x3;

				// Create a packed histogram 
				uint packedHistogram = 1U << (bitKey * 8);

				// Sum up all the packed keys (generates counted offsets up to current thread group)
				uint localSum = FFX_ParallelSort_BlockScanPrefix(packedHistogram, localID, laneID, waveLaneCount);

				// Last thread stores the updated histogram counts for the thread group
				// Scratch = 0xsum3|sum2|sum1|sum0 for thread group
				if (localID == (FFX_PARALLELSORT_THREADGROUP_SIZE - 1))
					gs_FFX_PARALLELSORT_LDSScratch[0] = localSum + packedHistogram;

				// Wait for everyone to catch up
				groupMemoryBarrierWithGroupSync();

				// Load the sums value for the thread group
				packedHistogram = gs_FFX_PARALLELSORT_LDSScratch[0];

				// Add prefix offsets for all 4 bit "keys" (packedHistogram = 0xsum2_1_0|sum1_0|sum0|0)
				packedHistogram = (packedHistogram << 8) + (packedHistogram << 16) + (packedHistogram << 24);

				// Calculate the proper offset for this thread's value
				localSum += packedHistogram;

				// Calculate target offset
				uint keyOffset = (localSum >> (bitKey * 8)) & 0xff;

				// Re-arrange the keys (store, sync, load)
				gs_FFX_PARALLELSORT_LDSSums[keyOffset] = localKey;
				groupMemoryBarrierWithGroupSync();
				localKey = gs_FFX_PARALLELSORT_LDSSums[localID];

				// Wait for everyone to catch up
				groupMemoryBarrierWithGroupSync();

#ifdef kRS_ValueCopy
				// Re-arrange the values if we have them (store, sync, load)
				gs_FFX_PARALLELSORT_LDSSums[keyOffset] = localValue;
				groupMemoryBarrierWithGroupSync();
				localValue = gs_FFX_PARALLELSORT_LDSSums[localID];

				// Wait for everyone to catch up
				groupMemoryBarrierWithGroupSync();
#endif // kRS_ValueCopy
			}

			// Need to recalculate the keyIndex on this thread now that values have been copied around the thread group
			uint keyIndex = (localKey >> ShiftBit) & 0xf;

			// Reconstruct histogram
			uint unused;
			interlockedAddGS(gs_FFX_PARALLELSORT_LocalHistogram[keyIndex], 1, unused);

			// Wait for everyone to catch up
			groupMemoryBarrierWithGroupSync();

			// Prefix histogram
			uint histogramPrefixSum = wavePrefixSum(localID < FFX_PARALLELSORT_SORT_BIN_COUNT ? interlockedLoadGS(gs_FFX_PARALLELSORT_LocalHistogram[localID]) : 0);

			// Broadcast prefix-sum via LDS
			if (localID < FFX_PARALLELSORT_SORT_BIN_COUNT)
				gs_FFX_PARALLELSORT_LDSScratch[localID] = histogramPrefixSum;

			// Get the global offset for this key out of the cache
			uint globalOffset = gs_FFX_PARALLELSORT_BinOffsetCache[keyIndex];

			// Wait for everyone to catch up
			groupMemoryBarrierWithGroupSync();

			// Get the local offset (at this point the keys are all in increasing order from 0 -> num bins in localID 0 -> thread group size)
			uint localOffset = localID - gs_FFX_PARALLELSORT_LDSScratch[keyIndex];

			// Write to destination
			uint totalOffset = globalOffset + localOffset;

			if (totalOffset < CBuffer.NumKeys)
			{
				DstBuffer[totalOffset] = localKey;

#ifdef kRS_ValueCopy
				DstPayload[totalOffset] = localValue;
#endif // kRS_ValueCopy
			}

			// Wait for everyone to catch up
			groupMemoryBarrierWithGroupSync();

			// Update the cached histogram for the next set of entries
			if (localID < FFX_PARALLELSORT_SORT_BIN_COUNT)
				gs_FFX_PARALLELSORT_BinOffsetCache[localID] += interlockedLoadGS(gs_FFX_PARALLELSORT_LocalHistogram[localID]);

			DataIndex += FFX_PARALLELSORT_THREADGROUP_SIZE;	// Increase the data offset by thread group size
		}
	}
}

uniform uint uShiftBit;
COMPUTE(FFX_PARALLELSORT_THREADGROUP_SIZE,1,1)
{
	const FFX_ParallelSortCB params = FFX_GetParams();
	FFX_ParallelSort_Scatter_uint( GROUP_THREAD_ID.x, GROUP_ID.x, WAVE_LANEINDEX, WAVE_LANECOUNT, params, uShiftBit );
}
